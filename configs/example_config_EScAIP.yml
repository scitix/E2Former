model:
  name: hydra
  pass_through_head_outputs: True
  otf_graph: True

  backbone:
    model: src.EScAIP.EScAIPBackbone

    # Global Configs
    activation: gelu                        # activation function in MLPs, all supported ["squared_relu", "gelu", "leaky_relu", "relu", "smelu", "star_relu"]
    direct_force: true                      # direct force or gradient force [Note: gradient force is only supported with "math" attention kernel]
    hidden_size: 128                        # hidden size of the model, devisable by 2 and num_heads
    batch_size: 12                          # batch size for the model, keep it the same as optim.batch_size
    use_fp16_backbone: false                # whether to use fp16 for the backbone or not
    use_compile: true                       # whether to use torch.compile or not (need to enable use_paddding)
    use_padding: true                       # whether to use padding or not


    # Molecular Graph Configs
    enforce_max_neighbors_strictly: true    # radius graph construction
    distance_function: gaussian             # distance function for edge distance features, all supported ["gaussian", "sigmoid", "linearsigmoid", "silu"]
    max_neighbors: 20                       # radius graph construction: maximum number of neighbors for each atom
    max_num_elements: 90                    # maximum number of elements in the dataset
    max_num_nodes_per_batch: 60             # used for padding, each batch will be padded to this size * batch_size
    max_radius: 6.0                         # radius graph construction: maximum radius for each atom
    otf_graph: true                         # whether to use on-the-fly graph construction or not
    use_pbc: true                           # whether to use periodic boundary conditions or not


    # Graph Neural Networks Configs
    atom_embedding_size: 128                # size of the atom embeddings
    atten_name: memory_efficient            # attention layer name, all supported ["memory_efficient", "math", "flash"]
    atten_num_heads: 8                      # number of attention heads
    edge_distance_embedding_size: 512       # size of the edge distance embeddings
    edge_distance_expansion_size: 600       # size of the edge distance expansion
    node_direction_embedding_size: 64       # size of the node direction embeddings
    node_direction_expansion_size: 10       # size of the node direction expansion
    num_layers: 6                           # number of layers in the model
    output_hidden_layer_multiplier: 2       # hidden layer multiplier for the output FFN
    readout_hidden_layer_multiplier: 2      # hidden layer multiplier for the readout FFN
    ffn_hidden_layer_multiplier: 2          # hidden layer multiplier for the FFNs in the main blocks
    use_angle_embedding: true               # whether to use angle embeddings in attention bias or not


    # Regularization Configs
    atten_dropout: 0.05                     # dropout rate for the attention layers
    mlp_dropout: 0.05                       # dropout rate for the MLPs
    normalization: rmsnorm                  # normalization layer, all supported ["rmsnorm", "layernorm", "skip"]
    stochastic_depth_prob: 0.0              # stochastic depth probability

  heads:
    forces:                                 # force head
      module: src.EScAIP.EScAIPDirectForceHead
    energy:                                 # energy head
      module: src.EScAIP.EScAIPEnergyHead
    stress:                                 # stress head
      module: src.EScAIP.EScAIPRank2Head
      output_name: stress
